version: '3.8'

services:
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    restart: unless-stopped
    networks:
      - data-net

  minio-client:
    image: minio/mc
    container_name: mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      mc alias set local http://minio:9000 minio minio123;
      mc mb -p local/bronze;
      mc mb -p local/silver;
      mc mb -p local/gold;
      mc cp /data/sample_data.csv local/bronze/sample_data.csv;
      tail -f /dev/null
      "
    volumes:
      - ./data:/data
    restart: unless-stopped
    networks:
      - data-net

  airflow-db:
    image: postgres:13
    container_name: airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    restart: unless-stopped
    networks:
      - data-net

  airflow:
    build:
      context: .
      dockerfile: ./airflow/Dockerfile
    container_name: airflow
    depends_on:
      - airflow-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'n6pKh7doDn1aAvRytD8RyrYY_nWvMEe3RCPnIoyM63E='
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/init:/opt/airflow/init
      - ./spark:/app
      - ./dbt_lakehouse:/dbt_lakehouse
      - ./dbt_lakehouse/profiles_template.yml:/home/airflow/.dbt/profiles.yml
      - ./data:/data
      - ./spark:/opt/spark
    ports:
      - "8080:8080"
      - "8091:8091"   # DBT Docs UI
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create --role Admin --username admin \
        --email admin@example.com --firstname Admin \
        --lastname User --password admin &&
      airflow scheduler & 
      airflow webserver"
    restart: unless-stopped
    networks:
      - data-net

  spark-master:
    image: bitnami/spark:3.4
    hostname: spark-master
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_METRICS_ENABLED=true
    ports:
      - "7077:7077"
      - "8081:8080"
    volumes:
      - ./spark:/app
      - ./data:/data
      - ./scripts:/scripts
    restart: unless-stopped
    networks:
      - data-net

  spark-worker-1:
    image: bitnami/spark:3.4
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master
    volumes:
      - ./spark:/app
      - ./data:/data
    restart: unless-stopped
    networks:
      - data-net

  spark-worker-2:
    image: bitnami/spark:3.4
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master
    volumes:
      - ./spark:/app
      - ./data:/data
    restart: unless-stopped
    networks:
      - data-net

  spark-thrift-server:
    build:
      context: .      # assuming your Dockerfile is here
      dockerfile: ./spark/Dockerfile
    container_name: spark-thrift-server
    ports:
      - "10001:10001"
      - "10000:10000"
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    volumes:
      - ./spark:/app
      - ./data:/data
    restart: unless-stopped
    networks:
      - data-net

  superset:
    build:
      context: .
      dockerfile: ./superset/Dockerfile
    container_name: superset
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_SECRET_KEY=mysecretkey
      - ADMIN_USERNAME=admin
      - ADMIN_FIRST_NAME=Admin
      - ADMIN_LAST_NAME=User
      - ADMIN_EMAIL=admin@example.com
      - ADMIN_PASSWORD=admin
      - DATABASE_URL=sqlite:////app/superset/superset.db
      - PATH=/app/.venv/bin:$PATH
      - PYTHONPATH=/app/.venv/lib/python3.10/site-packages
    volumes:
      - ./superset:/app/superset
    entrypoint: >
      /bin/bash -c "
      superset db upgrade &&
      if ! superset fab list-users | grep -q 'admin'; then
        superset fab create-admin \
          --username admin \
          --firstname Admin \
          --lastname User \
          --email admin@example.com \
          --password admin;
      fi &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088
      "
    restart: unless-stopped
    networks:
      - data-net

volumes:
  minio_data:

networks:
  data-net:
    driver: bridge